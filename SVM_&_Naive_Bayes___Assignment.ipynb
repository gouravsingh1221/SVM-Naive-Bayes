{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                                  S V M   &   Naive  Bayes\n",
        "                                    A S S I G N M E N T\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "3kU3S2wjAXoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "<BR>\n",
        "\n",
        "**Ans.-**\n",
        "A **Support Vector Machine (SVM)** is a supervised machine learning algorithm used for both **classification** and **regression** tasks, but it is most commonly used for **classification**.  \n",
        "It tries to find the **best boundary (hyperplane)** that separates different classes in the data.  \n",
        "\n",
        "---\n",
        "\n",
        "####  How It Works\n",
        "1. **Find the Optimal Hyperplane:**  \n",
        "   SVM searches for the hyperplane that **maximizes the margin** — the distance between the hyperplane and the nearest data points of each class.  \n",
        "   These closest data points are called **support vectors**.  \n",
        "\n",
        "2. **Maximizing the Margin:**  \n",
        "   The larger the margin, the better the model’s ability to generalize to unseen data.  \n",
        "\n",
        "3. **Non-linear Separation:**  \n",
        "   When data is not linearly separable, SVM uses the **kernel trick** to transform the data into a higher-dimensional space where it becomes linearly separable.  \n",
        "\n",
        "---\n",
        "\n",
        "####  Example\n",
        "If we want to classify emails as *Spam* or *Not Spam*:  \n",
        "- SVM plots the data points (emails) based on features like word frequency and link count.  \n",
        "- It then finds the line (or plane in higher dimensions) that best separates the two categories with the maximum possible margin.  \n",
        "\n",
        "---\n",
        "\n",
        "####  Summary\n",
        "- **Goal:** Find the best separating hyperplane between classes.  \n",
        "- **Support Vectors:** Data points closest to the boundary that influence its position.  \n",
        "- **Kernel Trick:** Allows SVM to handle non-linear data efficiently.  \n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ans.-**\n",
        "\n",
        "####  Hard Margin SVM\n",
        "- In **Hard Margin SVM**, the algorithm tries to find a **perfectly separating hyperplane** that divides the data into two classes **without any misclassification**.  \n",
        "- This means **all data points must lie on the correct side of the margin**.  \n",
        "- It assumes that the data is **linearly separable** and has **no noise or overlap** between classes.  \n",
        "\n",
        "**Key Characteristics:**\n",
        "- Strict separation — no data point can cross the margin.  \n",
        "- Can lead to **overfitting** if the data contains noise or outliers.  \n",
        "- Works only for perfectly separable data.  \n",
        "\n",
        " *Example:*  \n",
        "If we have two groups of points (say “red” and “blue”) that do not overlap at all, a hard margin SVM can draw a clean straight line between them with zero error.\n",
        "\n",
        "---\n",
        "\n",
        "####  Soft Margin SVM\n",
        "- **Soft Margin SVM** allows **some misclassifications** in order to achieve better generalization on unseen data.  \n",
        "- It introduces a **tolerance parameter (C)** that controls the trade-off between having a wider margin and fewer classification errors.  \n",
        "  - Small **C** → allows more misclassifications, focuses on a smooth margin.  \n",
        "  - Large **C** → focuses on correctly classifying all training points, may overfit.  \n",
        "\n",
        "**Key Characteristics:**\n",
        "- More flexible and robust with noisy or overlapping data.  \n",
        "- Works well for real-world datasets where perfect separation is rare.  \n",
        "\n",
        " *Example:*  \n",
        "In email classification, some emails might look slightly ambiguous — Soft Margin SVM allows these few errors to keep the model more general and less sensitive to noise.  \n",
        "\n",
        "---\n",
        "\n",
        "####  Summary\n",
        "| Aspect | Hard Margin SVM | Soft Margin SVM |\n",
        "|:--|:--|:--|\n",
        "| **Data Type** | Perfectly separable | Overlapping / noisy |\n",
        "| **Misclassification Allowed** | ❌ No | ✅ Yes |\n",
        "| **Generalization** | Low | High |\n",
        "| **Risk of Overfitting** | High | Lower |\n",
        "| **Parameter Used** | None | Regularization parameter (C) |\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ans.-**  The Kernel Trick in Support Vector Machines (SVM)\n",
        "\n",
        "####  What is the Kernel Trick?\n",
        "The **Kernel Trick** is a mathematical technique in SVM that allows the algorithm to handle **non-linearly separable data** by transforming it into a **higher-dimensional space** without explicitly computing the transformation.  \n",
        "\n",
        "In simple terms:  \n",
        "Instead of drawing a straight line in the original feature space, the kernel function helps SVM find a separating boundary (hyperplane) in a **new, higher-dimensional space** where the data becomes linearly separable.  \n",
        "\n",
        "---\n",
        "\n",
        "####  How It Works\n",
        "1. The kernel function calculates the **similarity** between two data points in the transformed space.  \n",
        "2. This allows SVM to operate in that higher-dimensional space **without actually performing** the heavy computation of transformation.  \n",
        "3. As a result, the model can handle complex data patterns efficiently.  \n",
        "\n",
        "---\n",
        "\n",
        "####  Example: Radial Basis Function (RBF) Kernel\n",
        "**Kernel Function:**\n",
        "\\[\n",
        "K(x_i, x_j) = \\exp(-\\gamma ||x_i - x_j||^2)\n",
        "\\]\n",
        "\n",
        "- The RBF kernel maps data into an infinite-dimensional space.  \n",
        "- It’s particularly effective for data that’s **non-linear or circularly separable**.  \n",
        "\n",
        "**Use Case Example:**  \n",
        "If we are classifying whether a point lies inside or outside a circle, a **linear kernel** would fail, but an **RBF kernel** can easily draw a circular boundary that separates the two classes.  \n",
        "\n",
        "---\n",
        "\n",
        "####  Common Types of Kernels\n",
        "| Kernel Type | Description | Use Case |\n",
        "|:--|:--|:--|\n",
        "| **Linear** | No transformation; used for linearly separable data | Text classification |\n",
        "| **Polynomial** | Adds polynomial terms of features | Image recognition |\n",
        "| **RBF (Gaussian)** | Maps data to infinite dimensions | Non-linear problems |\n",
        "| **Sigmoid** | Uses tanh function; similar to neural networks | Binary classification tasks |\n",
        "\n",
        "---\n",
        "\n",
        " **Summary:**  \n",
        "The **Kernel Trick** enables SVMs to efficiently solve complex, non-linear classification problems by using kernel functions — without explicitly transforming the data into higher dimensions.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ans.-**\n",
        "\n",
        "####  What is a Naïve Bayes Classifier?\n",
        "A **Naïve Bayes Classifier** is a probabilistic machine learning algorithm based on **Bayes’ Theorem**.  \n",
        "It is mainly used for **classification tasks**, such as spam detection, sentiment analysis, and text categorization.\n",
        "\n",
        "It predicts the class of a data point by calculating the **posterior probability** for each class and selecting the one with the highest probability.\n",
        "\n",
        "**Bayes’ Theorem:**\n",
        "\\[\n",
        "P(Y|X) = \\frac{P(X|Y) \\times P(Y)}{P(X)}\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- \\( P(Y|X) \\): Probability of class Y given data X (posterior)  \n",
        "- \\( P(X|Y) \\): Probability of data X given class Y (likelihood)  \n",
        "- \\( P(Y) \\): Prior probability of class Y  \n",
        "- \\( P(X) \\): Probability of the data X  \n",
        "\n",
        "---\n",
        "\n",
        "####  Why It’s Called “Naïve”\n",
        "It’s called **“naïve”** because the algorithm **assumes that all features are independent** of each other — meaning, one feature doesn’t affect another.  \n",
        "In real-world data, this assumption is rarely true, but it simplifies calculations and often works surprisingly well.\n",
        "\n",
        " *Example:*  \n",
        "In email spam detection, Naïve Bayes assumes that the presence of the word “offer” is independent of the word “discount,” even though they often appear together in spam emails.\n",
        "\n",
        "---\n",
        "\n",
        "####  Advantages\n",
        "- **Fast and efficient** for large datasets.  \n",
        "- Works well even with limited training data.  \n",
        "- Especially effective for **text classification** tasks.  \n",
        "\n",
        "---\n",
        "\n",
        "####  Limitation\n",
        "- The independence assumption can reduce accuracy when features are strongly correlated.  \n",
        "\n",
        "---\n",
        "\n",
        " **Summary:**  \n",
        "The Naïve Bayes Classifier applies Bayes’ Theorem with the simplifying (naïve) assumption of feature independence — making it simple, fast, and effective for many practical classification problems.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ans.-**  Variants of Naïve Bayes Classifier\n",
        "\n",
        "Naïve Bayes has several variants, each designed for different types of data.  \n",
        "The three most common are **Gaussian**, **Multinomial**, and **Bernoulli** Naïve Bayes.\n",
        "\n",
        "---\n",
        "\n",
        "####  1. Gaussian Naïve Bayes\n",
        "- **Used For:** Continuous (numeric) data.  \n",
        "- **Assumption:** Features follow a **normal (Gaussian) distribution**.  \n",
        "- The algorithm calculates the probability of a feature value using the Gaussian probability density function.  \n",
        "\n",
        "**Formula:**\n",
        "\\[\n",
        "P(x_i | y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\, e^{-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}}\n",
        "\\]\n",
        "\n",
        "**Example Use Case:**  \n",
        "Predicting the type of flower in the **Iris dataset** based on continuous features like petal length and width.\n",
        "\n",
        "---\n",
        "\n",
        "####  2. Multinomial Naïve Bayes\n",
        "- **Used For:** Discrete data such as **word counts or frequencies**.  \n",
        "- Commonly applied to text classification problems where features represent the number of times a word appears in a document.  \n",
        "- It works with features that represent counts or term frequencies (TF or TF-IDF values).\n",
        "\n",
        "**Example Use Case:**  \n",
        "Classifying emails as **spam or not spam** using word frequency counts.\n",
        "\n",
        "---\n",
        "\n",
        "####  3. Bernoulli Naïve Bayes\n",
        "- **Used For:** Binary or Boolean features (0s and 1s).  \n",
        "- It assumes that features are binary — meaning each feature is either **present (1)** or **absent (0)**.  \n",
        "- It considers whether a particular word or attribute exists rather than how many times it appears.\n",
        "\n",
        "**Example Use Case:**  \n",
        "Sentiment analysis where each word feature indicates whether it appears in a sentence or not (1 = present, 0 = absent).\n",
        "\n",
        "---\n",
        "\n",
        "####  Summary Table\n",
        "\n",
        "| Naïve Bayes Variant | Data Type | Example Use Case |\n",
        "|:--|:--|:--|\n",
        "| **GaussianNB** | Continuous / numeric | Iris flower classification |\n",
        "| **MultinomialNB** | Discrete counts / frequencies | Text or spam classification |\n",
        "| **BernoulliNB** | Binary / presence-absence | Sentiment analysis, document classification |\n",
        "\n",
        "---\n",
        "\n",
        " **Summary:**  \n",
        "- **GaussianNB:** for continuous numeric features.  \n",
        "- **MultinomialNB:** for count-based text data.  \n",
        "- **BernoulliNB:** for binary feature data.  \n",
        "Choosing the right variant ensures the model accurately reflects the data distribution and performs better on the task.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "###Dataset Info:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hU7ORaAqA-cB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXpq8obIAPAh",
        "outputId": "81e2688b-7f01-48fa-f939-dea384fda61c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Accuracy (Linear Kernel): 1.0\n",
            "\n",
            "Number of Support Vectors for Each Class: [ 3 11 11]\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# SVM Classifier on Iris Dataset using Linear Kernel\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Load the Iris dataset\n",
        "# -------------------------------\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Split into training and testing sets\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Train SVM Classifier with Linear Kernel\n",
        "# -------------------------------\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Make predictions and calculate accuracy\n",
        "# -------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Print results\n",
        "# -------------------------------\n",
        "print(\"SVM Classifier Accuracy (Linear Kernel):\", accuracy)\n",
        "print(\"\\nNumber of Support Vectors for Each Class:\", model.n_support_)\n",
        "print(\"\\nSupport Vectors:\\n\", model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "7_IDEB9OGLAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naïve Bayes on Breast Cancer Dataset\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "# -------------------------------\n",
        "cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y = cancer.target\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Split into training and testing sets\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Train Gaussian Naïve Bayes Model\n",
        "# -------------------------------\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Make predictions\n",
        "# -------------------------------\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Print classification report\n",
        "# -------------------------------\n",
        "print(\"Classification Report for Gaussian Naïve Bayes:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG-dKH_lGXfq",
        "outputId": "90e8e07a-e14d-4b9a-d7b6-c0b0f8179ef8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Gaussian Naïve Bayes:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "###Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "cBg9AmLUGoIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier on Wine Dataset with GridSearchCV (Tuning C and Gamma)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Load the Wine dataset\n",
        "# -------------------------------\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = wine.target\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Split into training and testing sets\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Define SVM model and parameter grid\n",
        "# -------------------------------\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Apply GridSearchCV for hyperparameter tuning\n",
        "# -------------------------------\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svm,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Evaluate the best model\n",
        "# -------------------------------\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Print results\n",
        "# -------------------------------\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Set Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueu4rjZLGy77",
        "outputId": "b88d9dd5-25ce-482f-f553-33f5f4686f83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best Cross-Validation Accuracy: 0.7179802955665024\n",
            "Test Set Accuracy with Best Model: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "###Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "RbN2dz6XHQMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier on Wine Dataset with GridSearchCV (Tuning C and Gamma)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Load the Wine dataset\n",
        "# -------------------------------\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = wine.target\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Split into training and testing sets\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Define SVM model and parameter grid\n",
        "# -------------------------------\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "svm = SVC()\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Apply GridSearchCV for hyperparameter tuning\n",
        "# -------------------------------\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=svm,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Evaluate the best model\n",
        "# -------------------------------\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Print results\n",
        "# -------------------------------\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "print(\"Test Set Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iplXdHUH6K6",
        "outputId": "8a676e25-b536-460e-e364-c7c3677e42f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Best Cross-Validation Accuracy: 0.7179802955665024\n",
            "Test Set Accuracy with Best Model: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "**Ans.-**\n",
        "### 📖 Email Classification Case Study — Spam vs. Not Spam\n",
        "\n",
        "As a data scientist for an email service company, my goal is to automatically classify emails as **Spam** or **Not Spam** using machine learning.  \n",
        "This involves data preprocessing, model selection, handling class imbalance, and evaluating performance.\n",
        "\n",
        "---\n",
        "\n",
        "###  1. Data Preprocessing\n",
        "\n",
        "####  Handling Missing Data\n",
        "- Some emails may have missing subjects or message bodies.  \n",
        "- Replace missing text fields with an empty string (`\"\"`) or a placeholder like `\"unknown\"`.  \n",
        "- Remove irrelevant columns (e.g., sender ID if not useful for spam detection).  \n",
        "\n",
        "####  Text Preprocessing\n",
        "1. **Tokenization:** Break text into individual words.  \n",
        "2. **Lowercasing:** Convert all words to lowercase for uniformity.  \n",
        "3. **Removing Stopwords:** Remove common words like “the”, “is”, “and” that don’t add meaning.  \n",
        "4. **Lemmatization/Stemming:** Reduce words to their root form (e.g., “offers” → “offer”).  \n",
        "\n",
        "#### Text Vectorization\n",
        "- Convert text into numerical format using:  \n",
        "  - **TF-IDF Vectorizer:** Gives importance to rare but meaningful words.  \n",
        "  - Or **CountVectorizer:** Converts text into a frequency-based feature matrix.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Model Selection\n",
        "\n",
        "####  Comparing SVM vs. Naïve Bayes\n",
        "| Model | Strengths | Limitations | When to Use |\n",
        "|:--|:--|:--|:--|\n",
        "| **Naïve Bayes** | Fast, simple, works well on text data | Assumes word independence | Ideal for large text datasets like emails |\n",
        "| **SVM** | Handles high-dimensional data well, powerful decision boundaries | Slower on large datasets | Best for smaller, well-balanced datasets |\n",
        "\n",
        " **Chosen Model:**  \n",
        "**Multinomial Naïve Bayes** — because:  \n",
        "- Text features (word counts) fit its assumptions.  \n",
        "- It’s fast and performs well even with a large number of features (vocabulary words).  \n",
        "\n",
        "---\n",
        "\n",
        "###  3. Handling Class Imbalance\n",
        "Since spam emails are fewer than legitimate ones, the dataset is **imbalanced**.  \n",
        "To address this:  \n",
        "- **Resampling Techniques:**  \n",
        "  - **Oversample minority class (spam)** using **SMOTE** or random oversampling.  \n",
        "  - Or **undersample majority class** to balance data.  \n",
        "- **Model-based Approach:**  \n",
        "  - Use class weights (`class_weight='balanced'` in SVM).  \n",
        "  - For Naïve Bayes, use prior probabilities adjusted to reflect class proportions.  \n",
        "\n",
        "---\n",
        "\n",
        "###  4. Model Evaluation\n",
        "\n",
        "####  Metrics to Use\n",
        "Accuracy alone isn’t enough for imbalanced data, so use:\n",
        "- **Precision:** % of emails predicted as spam that are actually spam.  \n",
        "- **Recall:** % of spam emails correctly identified.  \n",
        "- **F1-Score:** Balance between precision and recall.  \n",
        "- **ROC-AUC Score:** Measures model’s ability to distinguish between spam and not spam.  \n",
        "- **Confusion Matrix:** Visualizes correct vs. incorrect predictions.  \n",
        "\n",
        " *Goal:* Achieve **high recall** without letting too many legitimate emails be marked as spam (false positives).\n",
        "\n",
        "---\n",
        "\n",
        "###  5. Business Impact\n",
        "\n",
        "####  Benefits:\n",
        "- **Improved Productivity:** Reduces spam clutter, saving employees’ time.  \n",
        "- **Customer Trust:** Reliable spam detection improves user satisfaction.  \n",
        "- **Cost Efficiency:** Automated classification reduces manual review.  \n",
        "\n",
        "####  Key Consideration:\n",
        "- Maintain balance — being too aggressive may classify legitimate emails as spam, causing business communication loss.  \n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary**\n",
        "1. Clean and vectorize text data using TF-IDF.  \n",
        "2. Train a **Multinomial Naïve Bayes** model for speed and efficiency.  \n",
        "3. Handle imbalance with **SMOTE** or **class weighting**.  \n",
        "4. Evaluate using **precision, recall, F1-score, and ROC-AUC**.  \n",
        "5. The solution delivers a **scalable, accurate, and cost-effective** spam detection system that enhances user experience and trust.\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "X2sw7gNrISEA"
      }
    }
  ]
}